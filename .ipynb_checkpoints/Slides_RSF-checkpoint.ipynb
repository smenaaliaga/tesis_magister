{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22a1af70-52f7-4e25-a8c7-7285c7c8094d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## De Árboles de Decisión a  \n",
    "## Bosques Aleatorios de Supervivencia (RSF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efbc330-c7cb-4467-ab2b-1a2dd8406abb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Árboles de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165e0b94-8a78-4bd0-a6d4-ceaa33ee956a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Es un modelo de predicción basado en decisiones. Son intuitivos y sus decisiones son fáciles de interpretar (Modelo de caja blanca).\n",
    "\n",
    "Pueden ser utilizados para tareas de clasificación y regresión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e39d731-b1fd-4d55-8fe4-b6703dd06623",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<img src=\"img/iris_tree.png\" width=\"350\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0709242-fac4-446e-9839-0401842af19a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Impureza del nodo\n",
    "\n",
    "La impureza de un nodo nos aporta información acerca de la probabilidad de no obtener la clase predicha por un nodo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2f84d6-4258-456d-b8f4-09e46858d588",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "### Índice de Gini\n",
    "\n",
    "$ G_i = 1 \\sum_{i=1}^{n} {p_{i,k}^2} $\n",
    "\n",
    "Un nodo $i$-esimo es puro si $G_i = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb89c87-4b56-45d9-92f0-7ad11ac267c1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "### Entropía\n",
    "\n",
    "$ H_i = \\sum_{k=1}^2{p_{i,k} log_2(p_{i,k})} $\n",
    "\n",
    "Un nodo $i$-esimo es puro si $H_i = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c900290-efb7-41c1-8ff9-038c63b0ee88",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "$p_{i,k}$ es el ratio de instancias de clase $k$ entre las instancias de entrenamientos del nido $i$-esimo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d168f72f-a85f-40d1-a586-93cef74a7e21",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Entrenamiento del modelo: Algoritmo CART\n",
    "\n",
    "Se comienza diviendo el conjunto de entrenamiento en dos subconjuntos, mediante una característica $k$ y un umbral $t_k$.\n",
    "\n",
    "Ej. característcia ($k$:) longitud del pétalo iris; umbral ($t_k$): $\\leq$ 2.45 cm.\n",
    "\n",
    "### ¿Cómo se elige $k$ y $t_k$? \n",
    "\n",
    "Se selecciona ($k$, $t_k$) que produzca los nodos más puros (ponderados por tamaño), siguiendo la función de perdida:\n",
    "\n",
    "$ J(k,t_k) = \\frac{m_{izq}}{m} G_{izq} + \\frac{m_{dcha}}{m} G_{dcha}$\n",
    "\n",
    "$G_{izq/dcha}$ mide la impureza del subconjunto (nodo) izquierdo/derecha.\n",
    "\n",
    "$m_{izq/dcha}$ cantidad de instancias del subconjunto izquierda/derecha.\n",
    "\n",
    "Una vez el algoritmo divide el conjunto en dos subconjuntos, divide dichos subconjuntos siguiendo la misma lógica, después los subconjuntos sy así sucesivamente.\n",
    "\n",
    "Este proceso se repite hasta reducir completamente la impureza de los nodos o alcanzando la profundida máxima permitida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c04b65-5dc2-4267-a8bc-64f3da796670",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Complejidad Computacional\n",
    "\n",
    "Encontrar un árbol óptimo pertenece a los problemas NP-Completo. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df254ea3-3edf-4e68-aa19-a03cd0f60728",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Entrenar el modelo, resultado de comparar todas las características de todas las muestras de entrenamiento de cada nodo, requiere un tiempo $O(n * m * log_2(m))$.\n",
    "\n",
    "m: número de atributos; n: número de instancias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36071770-5a5a-40c8-be58-ad0d6b06d106",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Realizar una predicción, recorrer el árbols desde el nodo raíz hasta el nodo terminal, requiere un tiempo aproximado de $O(log_2(m))$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdebd62-10f6-4da9-b0de-29803e6b54dc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Supuestos e hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca84fbb8-f0d4-4a6c-990c-5c93302ae9fe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Los árboles de decisión asumen muy pocos supuesos sobre la distribución de los datos, se denominan a menudo \"modelos no paramétricos\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4801e83-e0b4-4587-96f6-3eb9349e2bc8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Para evitar el sobreajuste del modelo, se pueden utilizar hipterparámetros de regularización, si bien depende del algoritmo de entrenamiento utilizado, por lo general al menos se puede restringir la profundidadi máxima del árbol."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f38779f-8525-4761-8117-8cd923a32107",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Una implementación básica en Sckit-Learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a98a54-f07e-4524-8aee-a3fd44be69d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(\n",
    "    max_depth=2 # Profundidad máxima del árbol\n",
    ")\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f22e6c1-509f-4529-abc0-4f8329c76ed3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Ensamblaje y Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdeaad8-51ed-4c0a-b66b-29304d898fdb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "La idea básica detrás de los ensambladores es contar con diferentes modelos predictivos entrenados a partir de subconjuntos de un mismo conjunto de datos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5415b91-2ff0-481d-985b-70845c69cbf6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Los métodos más populares de ensamblaje son _bagging_, _boosting_ y _stacking_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d93cf2-657a-4aa3-8b2e-58b03a4ece36",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://miro.medium.com/max/4800/1*WLfYK7UUFgJEbNGMAwcRaQ.png\" width=\"550\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77187525-84f5-49e0-8146-eeb128942b45",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Claificadores por votación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a23471-6997-4afa-80c3-167d6b4f0a52",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Consiste en entrenar varios modelos (Regresión Logística, SVM, Árboles, etc.) a partir de un mismo conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557dd217-b081-4dba-ae43-91dbd941d5ba",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Para una nueva instancia, cada modelo genera una predicción. La predicción final del ensamblador será la predicción más votada de cada uno de los modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80906305-3332-45e1-ae32-dbcefc01731b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://i.stack.imgur.com/W7UmY.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681ddb1e-8279-497d-a5cb-da1d7dcd276c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Generamos datos dummy\n",
    "X, y = make_moons(n_samples=1000, noise=0.2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdee5a2c-87ba-41b2-af4e-9ca2d508c29e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "# Se crean las instancias de los modelos\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9ed3b8-c6d9-48b9-80f7-61cb604e96b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "# Se crea la instancia de un ensamblador por votación\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators = [('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting = 'hard'\n",
    ")\n",
    "# Se ajusta el modelo \n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7822ca92-381d-42d9-b70a-b59f3035d3e8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Rendimiento de los modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d683f79-bc7b-48c5-ae26-1d15736a296d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.87\n",
      "RandomForestClassifier 0.9466666666666667\n",
      "SVC 0.9566666666666667\n",
      "VotingClassifier 0.9533333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Rendimiento de cada uno de los modelos\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf) :\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fd0f02-a2e7-4448-9cf8-e7797100e01c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Bagging y Pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9578f8-cfa6-4e4f-a365-39c4bca86301",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Este enfoque crea un ensamblador de varios predictores a partir de un mismo algoritmo, y los entrena en diferentes subconjuntos aleatorios de un mismo conjunto de entrenamiento por cada predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af51f87-b434-4d61-b784-e97a6509dc17",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "El método __bagging__ muestrea subconjuntos con remplazo y __pasting__ lo hace sin remplazo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd63be9e-3665-4f23-9633-979fe54b5e9e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*iskng0M2Qv9GF0CADcl0Ww.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2f6844-f7e4-460f-a4e0-db85ea8168e2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Tal como se observa en la imagen, un ensamble con _bagging_ de 500 árboles es capaz de generalizar mucho mejor que un solo árbol. Posee un mayor sesgo pero compensado con una menor varianza.\n",
    "\n",
    "<img src=\"https://static.wixstatic.com/media/dcb8fd_0af0229fa2bb499d96a4efd2248c3c9c~mv2.png/v1/fill/w_1000,h_360,al_c,usm_0.66_1.00_0.01/dcb8fd_0af0229fa2bb499d96a4efd2248c3c9c~mv2.png\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3509d3ba-3ef0-4f92-815c-922c21b1c09f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6862ab-6749-443e-91fe-e9a539f5348d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "El _Random Forest_ es un ensamble de árboles de decisión, entrenados, por lo general, mediante método _bagging_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f7eebc-0d85-4d91-923e-c755ec1525a3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "En Sckit-Learn, la implementación de _Random Forest_ se especializa de la implementación con _Bagging Classifier_, debido a que al primero se le introduce una aleatoriedad extra cuando hacen crecer los árboles; en vez de buscar la mejor característica cuando divide un nodo, busca la mejor característica de un subconjunto de características. Esto permite más diversidad de árboles, compensando un sesgo más alto por una varianza más baja."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5132fbe3-2a67-4a28-90c2-b5da85ad4ee5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Implementación de Bagging Classifier en Sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cf770f-ad40-4bb8-a209-1cc41cdfba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(splitter = 'random', max_leaf_nodes = 16),\n",
    "    n_estimators = 500, max_samples = 1.0, bootstrap = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c049f63-2004-4586-85f0-cb1b5fd828c6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Implementación de Random Forest  en Sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8452a1c-5649-49bf-abd2-5395510a6d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier(\n",
    "    n_estimators=500, max_leaf_nodes=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabc5f94-659d-40db-9f80-db71e855e32a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Extremely Randomized Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c24e7a-f258-462e-b150-4412af4acabd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Al crear un árbol en _Random Forest_, en cada nodo solo se considera un subconjunto aleatorio de características ($k$) para la división. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61c81c5-8032-443a-bbbf-6684039c4ab9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Es posible hacer que los árboles sean aún más aleatorios al utilizar umbrales ($t_k$) aleatorios para cada característica ($k$), en vez de buscar los mejores umbrales posibles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b4f624-8d75-49cb-a299-89e53b5c88d2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "De esta forma se compensa aún más el sesgo con una varianza más baja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d236d32-1f80-426d-95e0-17738efeb539",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Importancia de la características"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3720169b-d3ff-4905-a6b5-b4863949c8c1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Una cualidad importante de los _Random Forest_ es que hacen posible medir la importancia de las características a través de cuanto reducen la impureza."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc116b2-d33d-4281-bd6d-1aa854f96655",
   "metadata": {},
   "source": [
    "$ J(k,t_k) = \\frac{m_{izq}}{m} G_{izq} + \\frac{m_{dcha}}{m} G_{dcha}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51943858-4153-4bd4-9494-38b496f49778",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    " Se escalan los resultados de manera que la suma de todas las importancias de cada característica sea 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1423aaa7-b57b-477a-b09d-02b2b59aee41",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1feeb2-55e4-48d0-a755-f8fde6420051",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators= 500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0d4b3ed-433e-4f75-bcf4-3c6f2d7e15a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.09101929129155692\n",
      "sepal width (cm) 0.022339654089200075\n",
      "petal length (cm) 0.4116565687809954\n",
      "petal width (cm) 0.4749844858382477\n"
     ]
    }
   ],
   "source": [
    "for name, score in zip(iris[\"feature_names\"], \n",
    "                       rnd_clf.feature_importances_) :\n",
    "    print(name, score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "rise": {
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
